ik分词器：在插件中进行安装  
1. ik_max_word最细粒度的拆分
2. ik_smart最粗粒度的拆分

创建索引的索引的时候进行绑定：存储的时候使用ik_max_word（更细粒度的存储），搜索的时候使用ik_smart（搜索的时候）
```
PUT /my_index
{ 
  "mapping": {
      "properties": {
          "text": {
              "type": "text"
              "analyzer": "ik_wax_word"
              "search_analyzer"："ik_smart"
            }
        }
    }
}

# IK 分词器配置文件
# 以下为自定义扩展词典文件，可以在其中添加新的词语，格式为：每行一个词语，可跟随一个或多个数字表示该词语的频率，默认值为 1。
# 自定义扩展词典文件可以是绝对路径或相对路径，如果是相对路径，则相对于 $ES_HOME/config 目录。
# 注意：修改了扩展词典文件之后，需要重新启动 Elasticsearch 才能生效。
# ========================================================
# 用户自定义词库
# 本示例采用相对路径，相对于 config 目录下的 ik 目录
#ik.analysis.custom_path=custom.dic

# IK 分词器配置
# ============================================
# 每次分词最多返回的词语数，默认值为 5。
#ik.max_word_length=5

# 是否启用智能分词模式，即 ik_smart 模式，默认值为 true。
#ik.use_smart=true

# 是否启用远程词典加载，即从远程服务器上下载词典数据，如启用则需要指定对应的 HTTP 服务地址，默认值为 false。
#ik.remote.enable=false
#ik.remote.url=http://localhost:8080/ik-analyzer-remote/

# 分词器插件配置
# ============================================
# 分词器相关的插件，可用于实现自定义分词规则、过滤器等，具体实现方式可以参考 Elasticsearch 官方文档和 IK 分词器源码。
#plugin.analysis.ik.type=org.elasticsearch.plugin.analysis.ik.AnalysisIkPlugin

```
## 设置ik分词器的设置文件
1. ik.xml文件中设置扩展字典，在远程设置扩展文件就可以进行读取
2. main.dic：日常中文语句中的词库
3. pre

## 自定义词库： （在总扩展文件中绑定my,dic文件）新建一个my.dic文件。定义一些扩展词典，重启分词器

