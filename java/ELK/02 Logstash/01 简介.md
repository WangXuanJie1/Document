## 
1. LogStash 是一个类似实时流水线的开源数据传输引擎，它像一个两头连接不同数据源的数据传输管道，将数据实时地从一个数据源传输到另一个数据源中。 在数据传输的过程中，LogStash 还可以对数据进行清洗、加工和整理，使数据在 到达目的地时直接可用或接近可用，为更复杂的数据分析、处理以及可视化做准备。
2. 既然需要将数据搬运到指定的地点，为什么不在数据产生时就将数据写到需要的地方呢?首先，许多数据在产生时并不支持直接写入到除本地文件以外的其他数据源。其次，在分布式环境下许多数据都 分散在不同容器甚至不同机器上，而在处理这些数据时往往需要将数据收集到一 起统一处理。 最后，即使软件支持将数据写入到指定的地点，但随着人们对数据理解的深入和新技术的诞生又会有新的数据分析需求出现，总会有一些接入是原生软件无法满足的。综上，LogStash 的核心价值就在于它将业务系统和数据展示系统隔离开来，屏蔽了各自系统变化对彼此的影响，使系统之间的依赖降低并可独自进化发展。
3. LogStash 可以从多个数据源提取数据，然后再清洗过滤并发送到指定的目标数据源。目标数据源也可以是多个，而且只要修改 LogStash 管道配置就可以轻松扩展数据的源头和目标。这在实际应用中非常有价值，尤其是在提取或发送的数据源发生变更时更为明显。比如原来只将数据提取到 Elasticsearch 中做检索，但现在需要将它们同时传给 Spark 做实时分析。如果事先没有使用 LogStash 就必须设计新代码向 Spark 发送数据，而如果预先使用了 LogStash 则只需要在管道配置中添加新的输出配置。这极大增强了数据传输的灵活性。

事件(Event) 是 Logstash 中另一个比较重要的概念， 它是对 Logstash 处理数据的一种面向对象的抽象。如果将 Logstash 比喻为管道，那么事件就是流淌在管道中的水流。事件由输入插件在读入数据时产生，不同输入插件产生事件的属性并不完全相同，但其中一定会包含有读入的原始数据。过滤器插件会对这些事件做进步处理， 处理的方式主要体现在对事件属性的访问、添加和修改。 最后，输出插件会将事件转换为目标数据源的数据格式并将它们存储到目标数据源中。

在 Logstash 管道中，每个输入插件都是在独立的线程中读取数据并产生事件。但输入插件并不会将事件直接传递给过滤器插件或输出插件，而是将事件写入到一个队列中。队列可以是基于内存的，也可以是基于硬盘的可持久化队列。 Logstash 管道会预先生成一组工作线程，这些线程会从队列中取出事件并在过滤器中处理，然后再通过输出插件将事件输出。事件队列可以协调输入插件与过滤器插件、输出插件的处理能力，使得 Logstash 具备了一定的消峰填谷能力。

除了输入插件使用事件队列与过滤器插件、输出插件交互以外，Logstash 还在输出插件与目标数据源之间提供了一个死信队列(Dead Letter Queue)。死信队列会在目标数据源没有成功接收到数据时，将失败的事件写入到队列中以供后续做进步处理。

输入插件、过滤器插件、输出插件、编解码器插件以及事件、队列等组件， 共同协作形成了整个 Logstash 的管道功能，它们构成 Logstash 的体系结构。

